{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMdhIaP7ZUlO5Pmv4z8nqU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/galua2001/colab/blob/main/trash2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nA28PHsxkqsI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85e09f29"
      },
      "source": [
        "# Task\n",
        "Develop a Python script using TensorFlow/Keras to train an image classification model on images located in the 'train' and 'test' folders within the 'my_dataset' directory on Google Drive. The 'train' and 'test' folders each contain 5 subfolders representing different image classes. After training and evaluating the model, create a function that takes an image file name from the 'trash' folder (also within 'my_dataset') as input and outputs the predicted class of that image. Finally, use this function to classify the images in the 'trash' folder and print the results. The script should handle mounting Google Drive to access the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fb0bef3"
      },
      "source": [
        "## Google drive 마운트\n",
        "\n",
        "### Subtask:\n",
        "Google Drive에 접근하기 위해 코랩에 마운트합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "290a34f8"
      },
      "source": [
        "**Reasoning**:\n",
        "Mount Google Drive to access the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff151df4",
        "outputId": "6d52a01b-5c62-44b7-dd40-f9ab186c84ab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b45990f"
      },
      "source": [
        "## 데이터 로드 및 전처리\n",
        "\n",
        "### Subtask:\n",
        "'train' 및 'test' 폴더에서 이미지를 로드하고, 딥러닝 모델 학습을 위한 형식으로 전처리합니다. 여기에는 이미지 크기 조정, 정규화, 데이터 증강 등이 포함될 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c87eb57"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries and define constants for image loading and preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "0ea4afd9",
        "outputId": "b8f7b5d8-8e15-437c-82b2-01acbd1ec7b8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomZoom\n",
        "\n",
        "# Define the base directory for your dataset on Google Drive\n",
        "DATASET_BASE_DIR = '/content/drive/MyDrive/my_dataset'\n",
        "\n",
        "# Define the paths for the 'train' and 'test' directories\n",
        "TRAIN_DIR = f'{DATASET_BASE_DIR}/train'\n",
        "TEST_DIR = f'{DATASET_BASE_DIR}/test'\n",
        "\n",
        "# Set parameters for image loading and preprocessing\n",
        "IMG_HEIGHT = 180\n",
        "IMG_WIDTH = 180\n",
        "BATCH_SIZE = 32\n",
        "SEED = 42"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.keras.layers.experimental'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3197127983.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage_dataset_from_directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomFlip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomZoom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the base directory for your dataset on Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.layers.experimental'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e5bbd94"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous import failed because `tensorflow.keras.layers.experimental.preprocessing` is deprecated. Import the preprocessing layers directly from `tensorflow.keras.layers`. Then, use `image_dataset_from_directory` to load the training and testing datasets and apply normalization and data augmentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTy-jVElluRD"
      },
      "source": [
        "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomZoom\n",
        "\n",
        "# Use image_dataset_from_directory to create training and testing datasets\n",
        "train_ds = image_dataset_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    seed=SEED)\n",
        "\n",
        "test_ds = image_dataset_from_directory(\n",
        "    TEST_DIR,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False, # No need to shuffle the test set\n",
        "    seed=SEED)\n",
        "\n",
        "# Normalize the pixel values of the images\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "# Apply data augmentation to the training dataset\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  RandomFlip(\"horizontal_and_vertical\"),\n",
        "  RandomRotation(0.2),\n",
        "  RandomZoom(0.2),\n",
        "])\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "# Configure the datasets for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print(\"Training dataset created with data augmentation and normalization.\")\n",
        "print(\"Testing dataset created with normalization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dcb9ae5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed because the variables `TRAIN_DIR`, `TEST_DIR`, `IMG_HEIGHT`, `IMG_WIDTH`, `BATCH_SIZE`, and `SEED` were not defined in the current session. Re-define these variables and then proceed with loading, preprocessing, and augmenting the datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBCxWBCClwoS"
      },
      "source": [
        "# Define the base directory for your dataset on Google Drive\n",
        "DATASET_BASE_DIR = '/content/drive/MyDrive/my_dataset'\n",
        "\n",
        "# Define the paths for the 'train' and 'test' directories\n",
        "TRAIN_DIR = f'{DATASET_BASE_DIR}/train'\n",
        "TEST_DIR = f'{DATASET_BASE_DIR}/test'\n",
        "\n",
        "# Set parameters for image loading and preprocessing\n",
        "IMG_HEIGHT = 180\n",
        "IMG_WIDTH = 180\n",
        "BATCH_SIZE = 32\n",
        "SEED = 42\n",
        "\n",
        "# Use image_dataset_from_directory to create training and testing datasets\n",
        "train_ds = image_dataset_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    seed=SEED)\n",
        "\n",
        "test_ds = image_dataset_from_directory(\n",
        "    TEST_DIR,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    interpolation='nearest',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False, # No need to shuffle the test set\n",
        "    seed=SEED)\n",
        "\n",
        "# Normalize the pixel values of the images\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "test_ds = test_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "# Apply data augmentation to the training dataset\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "  RandomFlip(\"horizontal_and_vertical\"),\n",
        "  RandomRotation(0.2),\n",
        "  RandomZoom(0.2),\n",
        "])\n",
        "\n",
        "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
        "\n",
        "# Configure the datasets for performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print(\"Training dataset created with data augmentation and normalization.\")\n",
        "print(\"Testing dataset created with normalization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "981c90a7"
      },
      "source": [
        "## 모델 구축\n",
        "\n",
        "### Subtask:\n",
        "이미지 분류를 위한 딥러닝 모델을 구축합니다. 사전 학습된 모델을 사용하거나 새로운 모델 아키텍처를 정의할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce1012c1"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary layers and the Sequential model class from TensorFlow Keras to define the model architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24bb6dad"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Rescaling\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d85c0c65"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a Sequential model and add convolutional, pooling, flatten, and dense layers according to the instructions. The Rescaling layer is not needed as normalization was applied during data loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a64b2ce8"
      },
      "source": [
        "num_classes = 5 # Based on the previous data loading output\n",
        "\n",
        "model = Sequential([\n",
        "  Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "  MaxPooling2D((2, 2)),\n",
        "  Conv2D(64, (3, 3), activation='relu'),\n",
        "  MaxPooling2D((2, 2)),\n",
        "  Conv2D(128, (3, 3), activation='relu'),\n",
        "  MaxPooling2D((2, 2)),\n",
        "  Flatten(),\n",
        "  Dense(128, activation='relu'),\n",
        "  Dropout(0.5),\n",
        "  Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5894dd32"
      },
      "source": [
        "## 모델 학습\n",
        "\n",
        "### Subtask:\n",
        "학습된 모델을 테스트 데이터로 평가하여 성능을 확인합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a552255"
      },
      "source": [
        "**Reasoning**:\n",
        "Compile and train the model using the specified optimizer, loss function, and metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3d7e6f1"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=EPOCHS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc5401f9"
      },
      "source": [
        "**Reasoning**:\n",
        "The model has been trained, now evaluate the model on the test dataset to check its performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cf0e7af"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f6557e7"
      },
      "source": [
        "## 예측 함수 생성\n",
        "\n",
        "### Subtask:\n",
        "'trash' 폴더의 이미지 파일 이름을 입력받아 해당 이미지의 분류 결과를 반환하는 함수를 생성합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea75f51b"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a function to classify a single image from the 'trash' folder using the trained model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7469557"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to get `class_names` from `train_ds` failed because it was prefetched. The `class_names` attribute is available on the original `image_dataset_from_directory` output before prefetching. I will redefine the function and include the `class_names` obtained from the initial datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7dc2444"
      },
      "source": [
        "## 예측 실행\n",
        "\n",
        "### Subtask:\n",
        "생성된 함수를 사용하여 'trash' 폴더의 이미지들을 분류합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bf78bd1"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the trash folder path using the base directory, list all image files within that folder, iterate through the files, classify each image using the previously defined function, and store the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55b60a38"
      },
      "source": [
        "## 결과 출력\n",
        "\n",
        "### Subtask:\n",
        "각 'trash' 이미지의 분류 결과를 출력합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e42fe9e2"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the classification_results dictionary and print each image file name and its predicted class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3efe0d7"
      },
      "source": [
        "## 특정 'trash' 이미지 예측 및 시각화\n",
        "\n",
        "### Subtask:\n",
        "사용자가 입력한 'trash' 이미지 파일 번호를 기반으로 예측을 수행하고 이미지와 결과를 함께 출력하는 함수를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyHQK1tPnO0C"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# Define the trash directory\n",
        "TRASH_DIR = f'{DATASET_BASE_DIR}/trash'\n",
        "\n",
        "def classify_trash_image(image_path):\n",
        "    \"\"\"\n",
        "    'trash' 폴더에 있는 단일 이미지를 분류합니다.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): 분류할 이미지 파일의 전체 경로.\n",
        "\n",
        "    Returns:\n",
        "        tuple: 예측된 클래스 이름 (str)과 각 클래스에 대한 예측 확률 (numpy array).\n",
        "    \"\"\"\n",
        "    img = image.load_img(image_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0) # 모델 예측을 위해 배치 차원 추가\n",
        "    img_array /= 255.0  # 이미지 정규화 (학습 시와 동일하게)\n",
        "\n",
        "    predictions = model.predict(img_array)\n",
        "    score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "    # Get class names from the training dataset\n",
        "    # Recreate a temporary dataset to get class names if train_ds was prefetched\n",
        "    if not hasattr(train_ds, 'class_names'):\n",
        "        temp_train_ds = image_dataset_from_directory(\n",
        "            TRAIN_DIR,\n",
        "            labels='inferred',\n",
        "            label_mode='int',\n",
        "            image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "            interpolation='nearest',\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False, # No need to shuffle just to get class names\n",
        "            seed=SEED)\n",
        "        class_names = temp_train_ds.class_names\n",
        "    else:\n",
        "        class_names = train_ds.class_names\n",
        "\n",
        "    predicted_class = class_names[np.argmax(score)]\n",
        "    return predicted_class, score.numpy()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6714c2c9"
      },
      "source": [
        "from IPython.display import display, Image\n",
        "import os\n",
        "import numpy as np # Import numpy for probability handling\n",
        "\n",
        "def predict_and_show_trash_image(file_number):\n",
        "    \"\"\"\n",
        "    'trash' 폴더에서 특정 이미지 파일 번호를 입력받아 예측을 수행하고 이미지를 표시합니다.\n",
        "\n",
        "    Args:\n",
        "        file_number (int): 'trash' 폴더 내 이미지 파일의 번호 (예: 59는 'trash_059.jpg').\n",
        "    \"\"\"\n",
        "    # 파일 이름 형식에 맞게 패딩하여 파일 이름 생성 (세 자릿수로 수정)\n",
        "    image_name = f'trash_{file_number:03d}.jpg'\n",
        "    image_path = os.path.join(TRASH_DIR, image_name)\n",
        "\n",
        "    # 파일이 존재하는지 확인\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"오류: '{image_name}' 파일을 찾을 수 없습니다. 파일 번호를 확인해주세요.\")\n",
        "        return\n",
        "\n",
        "    print(f\"'{image_name}' 이미지 분류를 시작합니다.\")\n",
        "\n",
        "    # 이미지 표시\n",
        "    print(\"이미지:\")\n",
        "    display(Image(filename=image_path))\n",
        "\n",
        "    # 이미지 분류 (이전 단계에서 정의된 classify_trash_image 함수 재사용)\n",
        "    predicted_class, probabilities = classify_trash_image(image_path) # Get both class and probabilities\n",
        "\n",
        "    print(f\"예측 결과: {predicted_class}\")\n",
        "\n",
        "    # Display probabilities\n",
        "    print(\"예측 확률:\")\n",
        "    # Get class names from the classify_trash_image function or redefine here if needed\n",
        "    # Assuming class_names is accessible or can be retrieved similarly\n",
        "    if not hasattr(train_ds, 'class_names'):\n",
        "        temp_train_ds = image_dataset_from_directory(\n",
        "            TRAIN_DIR,\n",
        "            labels='inferred',\n",
        "            label_mode='int',\n",
        "            image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "            interpolation='nearest',\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            seed=SEED)\n",
        "        class_names = temp_train_ds.class_names\n",
        "    else:\n",
        "        class_names = train_ds.class_names\n",
        "\n",
        "\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"  {class_name}: {probabilities[i]:.2f}\")\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# 사용 예시: trash_059.jpg 파일을 예측하고 표시합니다.\n",
        "# predict_and_show_trash_image(59)\n",
        "\n",
        "# 다른 예시: trash_007.jpg 파일을 예측하고 표시합니다.\n",
        "# predict_and_trash_image(7)\n",
        "\n",
        "# 분류하고 싶은 이미지 파일 번호를 아래 함수 호출에 넣어 사용하세요.\n",
        "# 예: trash_033.jpg를 분류하려면 predict_and_show_trash_image(33)을 호출합니다."
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37aedd63"
      },
      "source": [
        "**사용법**: 위의 코드 셀을 실행한 후, 새로 생성된 코드 셀에서 `predict_and_show_trash_image()` 함수를 호출하고 괄호 안에 예측하고자 하는 이미지 파일의 번호(예: trash_059.jpg 파일은 59)를 넣어 실행하면 이미지와 예측 결과를 확인하실 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "[링크 텍스트](https:// [링크 텍스트](https://))"
      ],
      "metadata": {
        "id": "BUiQ5EVpplfG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "4e47936c",
        "outputId": "a619798e-233a-4eb1-98a9-e184d0b913a5"
      },
      "source": [
        "predict_and_show_trash_image(30)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'trash_030.jpg' 이미지 분류를 시작합니다.\n",
            "이미지:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
            "Found 350 files belonging to 5 classes.\n",
            "예측 결과: plastic\n",
            "예측 확률:\n",
            "Found 350 files belonging to 5 classes.\n",
            "  cardboard: 0.16\n",
            "  glass: 0.20\n",
            "  metal: 0.23\n",
            "  paper: 0.16\n",
            "  plastic: 0.25\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "daa7590f",
        "outputId": "045bab53-7518-4b23-dca5-cb2283ae651c"
      },
      "source": [
        "import os\n",
        "\n",
        "TRASH_DIR = f'{DATASET_BASE_DIR}/trash' # Make sure TRASH_DIR is defined\n",
        "\n",
        "if os.path.exists(TRASH_DIR):\n",
        "    print(f\"'{TRASH_DIR}' 폴더의 파일 목록:\")\n",
        "    for filename in os.listdir(TRASH_DIR):\n",
        "        print(filename)\n",
        "else:\n",
        "    print(f\"오류: '{TRASH_DIR}' 폴더를 찾을 수 없습니다. 경로를 확인해주세요.\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/drive/MyDrive/my_dataset/trash' 폴더의 파일 목록:\n",
            "trash_059.jpg\n",
            "trash_007.jpg\n",
            "trash_067.jpg\n",
            "trash_085.jpg\n",
            "trash_087.jpg\n",
            "trash_083.jpg\n",
            "trash_030.jpg\n",
            "trash_082.jpg\n",
            "trash_048.jpg\n",
            "trash_098.jpg\n",
            "trash_013.jpg\n",
            "trash_058.jpg\n",
            "trash_086.jpg\n",
            "trash_021.jpg\n",
            "trash_041.jpg\n",
            "trash_074.jpg\n",
            "trash_015.jpg\n",
            "trash_043.jpg\n",
            "trash_010.jpg\n",
            "trash_033.jpg\n",
            "trash_034.jpg\n",
            "trash_006.jpg\n",
            "trash_102.jpg\n",
            "trash_068.jpg\n",
            "trash_045.jpg\n",
            "trash_061.jpg\n",
            "trash_094.jpg\n",
            "trash_028.jpg\n",
            "trash_079.jpg\n",
            "trash_017.jpg\n",
            "trash_057.jpg\n",
            "trash_035.jpg\n",
            "trash_038.jpg\n",
            "trash_062.jpg\n",
            "trash_097.jpg\n",
            "trash_003.jpg\n",
            "trash_065.jpg\n",
            "trash_011.jpg\n",
            "trash_016.jpg\n",
            "trash_047.jpg\n",
            "trash_081.jpg\n",
            "trash_037.jpg\n",
            "trash_026.jpg\n",
            "trash_029.jpg\n",
            "trash_032.jpg\n",
            "trash_014.jpg\n",
            "trash_044.jpg\n",
            "trash_092.jpg\n",
            "trash_100.jpg\n",
            "trash_019.jpg\n",
            "trash_056.jpg\n",
            "trash_096.jpg\n",
            "trash_002.jpg\n",
            "trash_046.jpg\n",
            "trash_049.jpg\n",
            "trash_012.jpg\n",
            "trash_066.jpg\n",
            "trash_060.jpg\n",
            "trash_027.jpg\n",
            "trash_084.jpg\n",
            "trash_077.jpg\n",
            "trash_009.jpg\n",
            "trash_099.jpg\n",
            "trash_004.jpg\n",
            "trash_080.jpg\n",
            "trash_031.jpg\n",
            "trash_040.jpg\n",
            "trash_064.jpg\n",
            "trash_093.jpg\n",
            "trash_091.jpg\n",
            "trash_022.jpg\n",
            "trash_024.jpg\n",
            "trash_078.jpg\n",
            "trash_076.jpg\n",
            "trash_073.jpg\n",
            "trash_020.jpg\n",
            "trash_104.jpg\n",
            "trash_018.jpg\n",
            "trash_008.jpg\n",
            "trash_070.jpg\n",
            "trash_101.jpg\n",
            "trash_005.jpg\n",
            "trash_071.jpg\n",
            "trash_089.jpg\n",
            "trash_053.jpg\n",
            "trash_025.jpg\n",
            "trash_088.jpg\n",
            "trash_095.jpg\n",
            "trash_050.jpg\n",
            "trash_069.jpg\n",
            "trash_036.jpg\n",
            "trash_072.jpg\n",
            "trash_023.jpg\n",
            "trash_063.jpg\n",
            "trash_103.jpg\n",
            "trash_001.jpg\n",
            "trash_042.jpg\n",
            "trash_075.jpg\n",
            "trash_054.jpg\n",
            "trash_055.jpg\n",
            "trash_039.jpg\n",
            "trash_090.jpg\n",
            "trash_052.jpg\n",
            "trash_051.jpg\n",
            "trash_130.jpg\n",
            "trash_129.jpg\n",
            "trash_114.jpg\n",
            "trash_108.jpg\n",
            "trash_105.jpg\n",
            "trash_134.jpg\n",
            "trash_120.jpg\n",
            "trash_117.jpg\n",
            "trash_107.jpg\n",
            "trash_128.jpg\n",
            "trash_109.jpg\n",
            "trash_122.jpg\n",
            "trash_136.jpg\n",
            "trash_121.jpg\n",
            "trash_111.jpg\n",
            "trash_127.jpg\n",
            "trash_118.jpg\n",
            "trash_112.jpg\n",
            "trash_115.jpg\n",
            "trash_126.jpg\n",
            "trash_135.jpg\n",
            "trash_131.jpg\n",
            "trash_133.jpg\n",
            "trash_132.jpg\n",
            "trash_124.jpg\n",
            "trash_116.jpg\n",
            "trash_113.jpg\n",
            "trash_106.jpg\n",
            "trash_123.jpg\n",
            "trash_137.jpg\n",
            "trash_119.jpg\n",
            "trash_110.jpg\n",
            "trash_125.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34e02741"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The model achieved a test accuracy of approximately 52.67% on the evaluation dataset.\n",
        "*   The classification results for the images in the 'trash' folder were successfully generated and printed, showing the predicted class for each image.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The model's test accuracy is relatively low (around 53%), suggesting it may not generalize well to new images. Further training with more epochs, exploring different model architectures, or fine-tuning a pre-trained model could potentially improve performance.\n",
        "*   Investigating the specific images that were misclassified in the 'trash' folder could provide insights into the model's weaknesses and inform strategies for data augmentation or model improvement.\n"
      ]
    }
  ]
}